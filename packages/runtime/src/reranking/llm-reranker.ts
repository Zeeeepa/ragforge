/**
 * LLM Reranker
 *
 * Uses an LLM to evaluate and rerank search results based on relevance
 * to the user's question.
 *
 * Features:
 * - Batching (group scopes for efficient token usage)
 * - Parallelization (process multiple batches concurrently)
 * - XML structured outputs (more reliable than JSON with small models)
 * - Query feedback and suggestions
 */

import { LuciformXMLParser } from '@luciformresearch/xmlparser';
import type { LLMProvider } from './llm-provider.js';
import type { SearchResult } from '../types/index.js';
import type { EntityContext, EntityField } from '../types/entity-context.js';
import { StructuredLLMExecutor } from '../llm/structured-llm-executor.js';
import type { LLMStructuredCallConfig, OutputSchema } from '../llm/structured-llm-executor.js';

export interface LLMRerankOptions {
  /**
   * Number of scopes per LLM request
   * Default: 10
   */
  batchSize?: number;

  /**
   * Maximum parallel LLM requests
   * Default: 5
   */
  parallel?: number;

  /**
   * Minimum relevance score to keep (0.0-1.0)
   * Default: 0.0 (keep all)
   */
  minScore?: number;

  /**
   * Maximum number of results to return after reranking
   * Default: undefined (return all)
   * Use: Limit results to top K most relevant scopes
   */
  topK?: number;

  /**
   * Request query improvement suggestions
   * Default: false
   */
  withSuggestions?: boolean;

  /**
   * Score merging strategy
   * Default: 'weighted'
   */
  scoreMerging?: 'weighted' | 'multiplicative' | 'llm-override';

  /**
   * Weights for score merging (if strategy = 'weighted')
   * Default: { vector: 0.3, llm: 0.7 }
   */
  weights?: {
    vector: number;
    llm: number;
  };

  /**
   * Debug: log generated prompts to console
   * Default: false
   */
  debugPrompt?: boolean;

  /**
   * Debug: skip LLM call and return mock evaluations
   * Default: false
   */
  mockup?: boolean;

  /**
   * Optional intention supplied by an agent orchestrating the search
   * (e.g., "Collect supporting evidence for OAuth flow")
   */
  agentIntention?: string;

  /**
   * Override or enrich operation metadata
   * @param results - Results after this operation
   * @param metadata - Default metadata generated by the operation
   * @returns Modified metadata
   */
  metadataOverride?: (results: any[], metadata: any) => any;
}

export interface ScopeEvaluation {
  scopeId: string;
  relevant: boolean;
  score: number;     // 0.0 - 1.0
  reasoning: string;
}

export interface QuerySuggestion {
  type: 'add_filter' | 'change_semantic' | 'expand_relationships' | 'other';
  description: string;
  exampleCode?: string;
}

export interface QueryFeedback {
  quality: 'excellent' | 'good' | 'insufficient' | 'poor';
  suggestions: QuerySuggestion[];
}

export interface LLMRerankResult {
  evaluations: ScopeEvaluation[];
  queryFeedback?: QueryFeedback;
}

export interface RerankInput {
  userQuestion: string;
  results: SearchResult[];
  queryContext?: string;
}

export class LLMReranker {
  private static defaultProvider?: LLMProvider;
  private entityContext: EntityContext;
  private executor: StructuredLLMExecutor; // NEW: Use unified executor
  // Increased from 400 to allow full code scopes in reranking prompts
  // With modern LLMs (8k+ context), we can afford to send more context per item
  private static readonly DEFAULT_FIELD_MAX = 2500;
  private static readonly DEFAULT_HEADER_MAX = 120;
  private static readonly DEFAULT_ARRAY_ITEMS = 5;

  /**
   * Get unique identifier for an entity
   * Uses configured uniqueField, falls back to numeric index
   */
  private getEntityId(entity: any, index: number): string {
    const uniqueField = this.entityContext.uniqueField;

    if (uniqueField) {
      const value = entity[uniqueField];
      if (value !== undefined && value !== null) {
        return String(value);
      }
    }
    // Fallback to numeric index
    return `__idx_${index}`;
  }

  /**
   * Set default LLM provider for all reranking operations
   */
  static setDefaultProvider(provider: LLMProvider) {
    this.defaultProvider = provider;
  }

  /**
   * Get the default LLM provider
   */
  static getDefaultProvider(): LLMProvider | undefined {
    return this.defaultProvider;
  }

  constructor(
    private llmProvider: LLMProvider,
    private options: LLMRerankOptions = {},
    entityContext: EntityContext
  ) {
    this.entityContext = entityContext;
    this.executor = new StructuredLLMExecutor(); // Initialize unified executor
  }

  /**
   * Preview the generated prompt without executing reranking
   * Useful for debugging and understanding how entities are presented to the LLM
   */
  previewPrompt(
    results: SearchResult[],
    userQuestion: string,
    queryContext?: string,
    maxResults: number = 3
  ): string {
    const batch = results.slice(0, maxResults);
    return this.buildPrompt(batch, userQuestion, queryContext, false);
  }

  /**
   * NEW: Rerank using StructuredLLMExecutor (unified approach)
   */
  async rerankWithExecutor(input: RerankInput): Promise<LLMRerankResult> {
    const {
      batchSize = 10,
      parallel = 5,
      minScore = 0.0,
      topK,
      withSuggestions = false
    } = this.options;

    // Extract entities from SearchResults
    const entities = input.results.map(r => r.entity);

    // Use StructuredLLMExecutor for reranking
    const { evaluations, queryFeedback } = await this.executor.executeReranking(entities, {
      userQuestion: input.userQuestion,
      entityContext: this.entityContext,
      llmProvider: this.llmProvider,
      batchSize,
      parallel,
      withFeedback: withSuggestions,
      getItemId: (entity, index) => this.getEntityId(entity, index),
      contextData: input.queryContext ? { queryContext: input.queryContext } : undefined
    });

    // Convert ItemEvaluation to ScopeEvaluation (same structure)
    const scopeEvaluations: ScopeEvaluation[] = evaluations
      .filter(e => e.score >= minScore)
      .map(e => ({
        scopeId: e.id,
        relevant: e.relevant ?? true,
        score: e.score,
        reasoning: e.reasoning
      }));

    // Sort and limit
    scopeEvaluations.sort((a, b) => b.score - a.score);
    const limited = topK !== undefined && topK > 0
      ? scopeEvaluations.slice(0, topK)
      : scopeEvaluations;

    return {
      evaluations: limited,
      queryFeedback
    };
  }

  /**
   * LEGACY: Original rerank implementation (kept for backward compatibility)
   * TODO: Eventually deprecate in favor of rerankWithExecutor
   */
  async rerank(input: RerankInput): Promise<LLMRerankResult> {
    // For now, delegate to new implementation
    return this.rerankWithExecutor(input);

    /* ORIGINAL IMPLEMENTATION COMMENTED OUT - can be removed after testing
    const {
      batchSize = 10,
      parallel = 5,
      minScore = 0.0,
      topK,
      withSuggestions = false
    } = this.options;

    // 1. Split results into batches
    const batches = this.createBatches(input.results, batchSize);

    // 2. Process batches in parallel with limit
    const allEvaluations: ScopeEvaluation[] = [];
    let queryFeedback: QueryFeedback | undefined;
    const agentIntention = this.options.agentIntention;

    for (let i = 0; i < batches.length; i += parallel) {
      const batchGroup = batches.slice(i, i + parallel);
      const firstInGroup = i === 0;

      if (typeof this.llmProvider.generateBatch === 'function') {
        const promptMetas = batchGroup.map((batch, idx) => ({
          batch,
          includeSuggestions: withSuggestions && firstInGroup && idx === 0
        }));

        const prompts = promptMetas.map(meta => {
          const prompt = this.buildPrompt(
            meta.batch,
            input.userQuestion,
            input.queryContext,
            meta.includeSuggestions,
            agentIntention
          );

          if (this.options.debugPrompt) {
            console.log('\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
            console.log('ðŸ” LLM RERANKER PROMPT (Batch)');
            console.log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
            console.log(prompt);
            console.log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n');
          }

          return prompt;
        });

        const responses = await this.llmProvider.generateBatch(prompts);

        responses.forEach((response, idx) => {
          if (this.options.debugPrompt) {
            console.log('\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
            console.log('ðŸ¤– LLM RERANKER RESPONSE (Batch)');
            console.log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
            console.log(response);
            console.log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n');
          }
          const { evaluations, queryFeedback: feedback } = this.parseResponse(response, promptMetas[idx].batch);
          allEvaluations.push(...evaluations);
          if (!queryFeedback && feedback) {
            queryFeedback = feedback;
          }
        });
      } else {
        const promises = batchGroup.map((batch, idx) =>
          this.evaluateBatch(
            batch,
            input.userQuestion,
            input.queryContext,
            withSuggestions && firstInGroup && idx === 0,
            agentIntention
          )
        );

        const results = await Promise.all(promises);
        results.forEach(r => {
          allEvaluations.push(...r.evaluations);
          if (!queryFeedback && r.queryFeedback) {
            queryFeedback = r.queryFeedback;
          }
        });
      }
    }

    // 3. Filter by minimum score
    let filtered = allEvaluations.filter(e => e.score >= minScore);

    // 4. Sort by score (descending) and limit to topK if specified
    filtered.sort((a, b) => b.score - a.score);
    if (topK !== undefined && topK > 0) {
      filtered = filtered.slice(0, topK);
    }

    return {
      evaluations: filtered,
      queryFeedback
    };
    */
  }

  private createBatches(results: SearchResult[], size: number): SearchResult[][] {
    const batches: SearchResult[][] = [];
    for (let i = 0; i < results.length; i += size) {
      batches.push(results.slice(i, i + size));
    }
    return batches;
  }

  private async evaluateBatch(
    batch: SearchResult[],
    userQuestion: string,
    queryContext?: string,
    withSuggestions: boolean = false,
    agentIntention?: string
  ): Promise<LLMRerankResult> {
    // Mockup mode: skip LLM call and return mock evaluations for debugging
    if (this.options.mockup) {
      console.log('âš ï¸  MOCKUP MODE: Skipping LLM call, returning mock evaluations');
      const evaluations: ScopeEvaluation[] = batch.map((result, idx) => ({
        scopeId: this.getEntityId(result.entity, idx),
        relevant: true,
        score: 0.9 - (idx * 0.05), // Decreasing scores
        reasoning: `MOCK: This is mock reasoning for item ${idx}`
      }));
      return { evaluations };
    }

    const prompt = this.buildPrompt(batch, userQuestion, queryContext, withSuggestions, agentIntention);

    // Debug: log prompt if enabled
    if (this.options.debugPrompt) {
      console.log('\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
      console.log('ðŸ” LLM RERANKER PROMPT');
      console.log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
      console.log(prompt);
      console.log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n');
    }

    const response = await this.llmProvider.generateContent(prompt);

    // Debug: log response if enabled
    if (this.options.debugPrompt) {
      console.log('\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
      console.log('ðŸ¤– LLM RERANKER RESPONSE');
      console.log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
      console.log(response);
      console.log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n');
    }

    return this.parseResponse(response, batch);
  }

  private buildPrompt(
    batch: SearchResult[],
    userQuestion: string,
    queryContext?: string,
    withSuggestions: boolean = false,
    agentIntention?: string
  ): string {
    let prompt = `You are ranking ${this.entityContext.displayName} for relevance.\n\nUser question: "${userQuestion}"\n`;

    if (agentIntention) {
      prompt += `Agent intention: "${agentIntention}"\n`;
    }

    if (queryContext) {
      prompt += `
Query context:
${queryContext}
`;
    }

    prompt += `
${this.entityContext.displayName} to evaluate:

`;

    batch.forEach((result, idx) => {
      const entity = result.entity;

      // Render required fields (typically in header line)
      const requiredFields = this.entityContext.fields.filter(f => f.required);
      const headerParts = requiredFields.map(field => {
        const rawValue = (entity as any)[field.name];
        if (this.shouldSkipField(field.name, rawValue)) {
          return field.name;
        }
        const headerValue = this.formatValue(rawValue, field, LLMReranker.DEFAULT_HEADER_MAX);
        return headerValue ?? field.name;
      });
      prompt += `[${idx}] ${headerParts.join(' - ')}
`;

      // Render optional fields
      const optionalFields = this.entityContext.fields.filter(f => !f.required);
      optionalFields.forEach(field => {
        // Use getFieldValue to prefer summary if configured
        const value = this.getFieldValue(entity, field);
        if (!value || this.shouldSkipField(field.name, value)) {
          return;
        }

        const label = field.label || field.name;

        const printable = this.formatArrayOrValue(value, field);
        if (printable) {
          prompt += `${label}: ${printable}
`;
        }
      });

      // Render enrichment fields (relationships)
      this.entityContext.enrichments.forEach(enrichment => {
        const value = (entity as any)[enrichment.fieldName];
        if (value && Array.isArray(value) && value.length > 0) {
          const maxItems = enrichment.maxItems || 10;
          const items = value.slice(0, maxItems);
          prompt += `${enrichment.label}: ${items.join(', ')}
`;
        }
      });

      prompt += `
`;
    });

    prompt += `

IMPORTANT: You MUST respond with XML ONLY. Do NOT use JSON. Do NOT use markdown code blocks.

Instructions:
- Evaluate each ${this.entityContext.displayName} item's relevance to the user question
- Score must be a single decimal number from 0.0 to 1.0
- For the "reason" attribute: BE SPECIFIC! Reference concrete details from BOTH:
  1. The user's question (specific keywords, concepts they asked about)
  2. The item being evaluated (specific names, properties, context)
- Each reason must mention at least one property unique to that item (title, description, id, etc.). Do NOT repeat the exact same sentence for multiple items.
- Explicitly refer to the item by its own name/title from the list (e.g., "[2] Harry Potter"). Do not describe different items when scoring a given entry.
- AVOID generic phrases like "directly related" or "addresses the question"
- INSTEAD, explain HOW and WHY using specific details
- Output raw XML without any formatting or wrapping

Example XML format (notice how reasons are SPECIFIC):
<evaluations>
  <item id="0" score="0.9" reason="references the same keywords the user asked about and explains the overlap" />
  <item id="1" score="0.2" reason="discusses an unrelated topic; the user query keywords never appear here" />
</evaluations>

Your XML response:`;

    if (withSuggestions) {
      prompt += `

Include query feedback in the same XML:

<evaluations>
  <item id="0" score="0.8" reason="..." />
  <item id="1" score="0.3" reason="..." />
  <feedback quality="excellent|good|insufficient|poor">
    <suggestion type="add_filter|change_semantic|expand_relationships">
      description here
    </suggestion>
  </feedback>
</evaluations>`;
    }

    return prompt;
  }

  /**
   * Get the appropriate field value, preferring summary if configured
   */
  private getFieldValue(entity: any, field: EntityField): unknown {
    // If preferSummary is enabled, try to reconstruct summary from individual fields
    if (field.preferSummary) {
      const prefix = `${field.name}_summary_`;
      const summaryFields = Object.keys(entity).filter(k => k.startsWith(prefix));

      if (summaryFields.length > 0) {
        // Reconstruct summary object from individual fields
        const summary: any = {};

        for (const key of summaryFields) {
          const fieldName = key.substring(prefix.length);

          // Skip metadata fields
          if (fieldName === 'hash' || fieldName.endsWith('_at')) {
            continue;
          }

          const value = entity[key];

          // Parse comma-separated arrays back to arrays
          if (typeof value === 'string' && value.includes(',')) {
            summary[fieldName] = value.split(',').map(s => s.trim());
          } else {
            summary[fieldName] = value;
          }
        }

        // Only use summary if it has actual content
        if (Object.keys(summary).length > 0) {
          return summary;
        }
      }
    }

    // Fall back to original field
    return entity[field.name];
  }

  private shouldSkipField(fieldName: string, value: unknown): boolean {
    const lower = fieldName.toLowerCase();
    if (lower.includes('embedding') || lower.includes('vector')) {
      return true;
    }

    if (Array.isArray(value)) {
      if (value.length === 0) {
        return false;
      }
      return value.every(item => typeof item === 'number');
    }

    return false;
  }

  private truncate(value: string, maxLength?: number): string {
    const limit = maxLength ?? LLMReranker.DEFAULT_FIELD_MAX;
    if (value.length <= limit) {
      return value;
    }
    return value.slice(0, limit - 1) + 'â€¦';
  }

  /**
   * Format structured summary objects (with purpose, operation, dependency, etc.)
   */
  private formatStructuredSummary(summary: any): string {
    const parts: string[] = [];

    if (summary.purpose) {
      parts.push(`Purpose: ${summary.purpose}`);
    }

    if (summary.operation && Array.isArray(summary.operation) && summary.operation.length > 0) {
      parts.push(`Operations: ${summary.operation.join('; ')}`);
    }

    if (summary.dependency && Array.isArray(summary.dependency) && summary.dependency.length > 0) {
      parts.push(`Dependencies: ${summary.dependency.join(', ')}`);
    }

    if (summary.concept && Array.isArray(summary.concept) && summary.concept.length > 0) {
      parts.push(`Concepts: ${summary.concept.join(', ')}`);
    }

    if (summary.complexity) {
      parts.push(`Complexity: ${summary.complexity}`);
    }

    if (summary.suggestion && Array.isArray(summary.suggestion) && summary.suggestion.length > 0) {
      parts.push(`Suggestions: ${summary.suggestion.join('; ')}`);
    }

    return parts.join('\n');
  }

  private formatValue(value: unknown, field: EntityField, fallbackMax: number = LLMReranker.DEFAULT_FIELD_MAX): string | null {
    if (typeof value === 'string') {
      return this.truncate(value, field.maxLength ?? fallbackMax);
    }
    if (typeof value === 'number' || typeof value === 'boolean') {
      return String(value);
    }
    // Handle structured summaries (JSON objects with purpose, operation, etc.)
    if (typeof value === 'object' && value !== null && !Array.isArray(value)) {
      return this.formatStructuredSummary(value);
    }
    return null;
  }

  private formatArrayOrValue(value: unknown, field: EntityField): string | null {
    if (Array.isArray(value)) {
      const printable = value
        .filter(item => typeof item === 'string')
        .slice(0, field.maxItems ?? LLMReranker.DEFAULT_ARRAY_ITEMS)
        .map(item => this.truncate(item as string, field.maxLength))
        .join(', ');
      return printable || null;
    }
    return this.formatValue(value, field);
  }

  private parseResponse(response: string, batch: SearchResult[]): LLMRerankResult {
    try {
      // Extract XML from response (remove markdown code blocks if present)
      let xmlText = response.trim();

      if (xmlText.includes('```')) {
        // Remove markdown code blocks
        const match = xmlText.match(/```(?:xml)?\s*\n?([\s\S]*?)\n?```/);
        if (match && match[1]) {
          xmlText = match[1].trim();
        } else {
          // Fallback: remove lines with backticks
          const lines = xmlText.split('\n');
          xmlText = lines.filter(line => !line.includes('```')).join('\n').trim();
        }
      }

      // Parse XML using LuciformXMLParser
      const parser = new LuciformXMLParser(xmlText, { mode: 'luciform-permissive' });
      const result = parser.parse();

      if (!result.document?.root) {
        throw new Error('No XML root element found');
      }

      const root = result.document.root;

      // Find all <item> elements (generic for any entity type)
      const itemElements = root.children?.filter(
        (child: any) => child.type === 'element' && child.name === 'item'
      ) || [];

      // Parse evaluations
      const evaluations: ScopeEvaluation[] = itemElements.map((itemEl: any, idx) => {
        // Attributes are stored in a Map, use .get() to access them
        const attrs = itemEl.attributes as Map<string, string>;
        const id = parseInt(attrs.get('id') || '0');

        // Parse score (handle ranges like "0.2-0.5" by taking the max value)
        const scoreAttr = attrs.get('score') || '0.0';
        let score = 0.0;
        if (scoreAttr.includes('-')) {
          // Range: take the max value
          const parts = scoreAttr.split('-').map((s: string) => parseFloat(s.trim()));
          score = Math.max(...parts);
        } else {
          score = parseFloat(scoreAttr);
        }

        const reason = attrs.get('reason') || '';

        if (id >= batch.length) {
          return null;
        }

        // Use the getEntityId helper to get a stable unique identifier
        const entityId = this.getEntityId(batch[id].entity, id);

        const evaluation = {
          scopeId: entityId,
          relevant: score > 0.5,
          score: score,
          reasoning: reason
        };

        return evaluation;
      }).filter(Boolean) as ScopeEvaluation[];

      // Parse feedback if present
      let queryFeedback: QueryFeedback | undefined;
      const feedbackEl = root.children?.find(
        (child: any) => child.type === 'element' && child.name === 'feedback'
      );

      if (feedbackEl) {
        const feedbackAttrs = (feedbackEl as any).attributes as Map<string, string>;
        const quality = feedbackAttrs.get('quality') || 'unknown';

        const suggestionElements = feedbackEl.children?.filter(
          (child: any) => child.type === 'element' && child.name === 'suggestion'
        ) || [];

        const suggestions: QuerySuggestion[] = suggestionElements.map((suggEl: any) => {
          const suggAttrs = suggEl.attributes as Map<string, string>;
          const type = suggAttrs.get('type') || 'other';
          const textContent = suggEl.children
            ?.filter((c: any) => c.type === 'text')
            ?.map((c: any) => c.content)
            ?.join('') || '';

          return {
            type: type as QuerySuggestion['type'],
            description: textContent.trim()
          };
        });

        queryFeedback = {
          quality: quality as QueryFeedback['quality'],
          suggestions
        };
      }

      return { evaluations, queryFeedback };

    } catch (error: any) {
      // Fallback: return neutral scores
      console.warn('Failed to parse LLM response:', error.message);
      console.warn('Response:', response);

      return {
        evaluations: batch.map(r => ({
          scopeId: r.entity.uuid,
          relevant: true,
          score: 0.5, // Neutral score
          reasoning: 'Failed to parse LLM response'
        }))
      };
    }
  }

  /**
   * Merge LLM scores with existing scores
   *
   * Returns ONLY results that have an LLM evaluation.
   * If topK was specified in rerank(), this will naturally limit results.
   */
  mergeScores(
    results: SearchResult[],
    evaluations: ScopeEvaluation[],
    strategy: LLMRerankOptions['scoreMerging'] = 'weighted',
    weights: { vector: number; llm: number } = { vector: 0.3, llm: 0.7 }
  ): SearchResult[] {
    const evalMap = new Map(evaluations.map(e => [e.scopeId, e]));

    // Only keep results that have an LLM evaluation
    return results
      .map((result, index) => {
        const entityId = this.getEntityId(result.entity, index);
        return { result, entityId, index };
      })
      .filter(({ entityId }) => evalMap.has(entityId))
      .map(({ result, entityId, index }) => {
        const evaluation = evalMap.get(entityId)!;

        let finalScore: number;

        switch (strategy) {
          case 'weighted':
            finalScore = weights.vector * result.score + weights.llm * evaluation.score;
            break;

          case 'multiplicative':
            finalScore = result.score * evaluation.score;
            break;

          case 'llm-override':
            // Use LLM if very confident, otherwise weighted
            finalScore = evaluation.score > 0.9
              ? evaluation.score
              : 0.5 * result.score + 0.5 * evaluation.score;
            break;

          default:
            finalScore = weights.vector * result.score + weights.llm * evaluation.score;
        }

        return {
          ...result,
          score: finalScore,
          scoreBreakdown: {
            ...result.scoreBreakdown,
            vector: result.score,
            llm: evaluation.score,
            llmReasoning: evaluation.reasoning
          }
        };
      });
  }
}
